{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "#import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "#matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The class of dueling DQN with three convolutional layers\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size,action_num):\n",
    "        #The network recieves a state from the sumo, flattened into an array.\n",
    "        #It then resizes it and processes it through three convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,3600,2],dtype=tf.float32)\n",
    "        self.legal_actions =  tf.placeholder(shape=[None,action_num],dtype=tf.float32)\n",
    "        \n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,60,60,2])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[4,4],stride=[2,2],padding='VALID', activation_fn=self.relu, biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[2,2],stride=[1,1],padding='VALID', activation_fn=self.relu, biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=128,kernel_size=[2,2],stride=[1,1],padding='VALID', activation_fn=self.relu, biases_initializer=None)\n",
    "\n",
    "        #It is split into Value and Advantage\n",
    "        self.stream = slim.flatten(self.conv3)\n",
    "        self.stream0 = slim.fully_connected(self.stream, 128, activation_fn=self.relu)\n",
    "    \n",
    "        self.streamA = self.stream0\n",
    "        self.streamV = self.stream0\n",
    "        \n",
    "        \n",
    "        self.streamA0 = slim.fully_connected(self.streamA,h_size, activation_fn=self.relu)\n",
    "        self.streamV0 = slim.fully_connected(self.streamV, h_size, activation_fn=self.relu)\n",
    "        \n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        action_num = np.int32(action_num)\n",
    "        self.AW = tf.Variable(xavier_init([h_size,action_num]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA0,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV0,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout0 = self.Value  + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        #The final Q value is the addition of the Q value and penelized value for illegal actions\n",
    "        self.Qout = tf.add(self.Qout0, self.legal_actions)\n",
    "        #The predicted action\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the mean square error between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,np.int32(action_num),dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "#         self.Q = tf.reduce_sum(self.Qout, axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "    def relu(self, x, alpha=0.01, max_value=None):\n",
    "        '''ReLU.\n",
    "\n",
    "        alpha: slope of negative section.\n",
    "        '''\n",
    "        negative_part = tf.nn.relu(-x)\n",
    "        x = tf.nn.relu(x)\n",
    "        if max_value is not None:\n",
    "            x = tf.clip_by_value(x, tf.cast(0., dtype=tf.float32),\n",
    "                                 tf.cast(max_value, dtype=tf.float32))\n",
    "        x -= tf.constant(alpha, dtype=tf.float32) * negative_part\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The normal experience buffer\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "        print \"ADDED\", len(self.buffer)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        print \"BUFFer:\", len(self.buffer)\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The target update functions\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The parameters\n",
    "batch_size = 128 #How many experiences to use for each training step.\n",
    "update_freq = 1 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.01 #Final chance of random action\n",
    "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 1500#000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 2000#0000 #How many steps of random actions before training begins.\n",
    "max_epLength = 500 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "action_num = 9 #total number of actions\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 64 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class priorized_experience_buffer():\n",
    "    def __init__(self, buffer_size = 20000):\n",
    "        self.buffer = []\n",
    "        self.prob = []\n",
    "        self.err = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.alpha = 0.2\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.err[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            self.prob[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "        self.err.extend([10000]*len(experience))\n",
    "        self.prob.extend([1]*len(experience))\n",
    "            \n",
    "    def updateErr(self, indx, error):\n",
    "        for i in range(0, len(indx)):\n",
    "            self.err[indx[i]] = math.sqrt(error[i])\n",
    "        r_err = ss.rankdata(self.err)  #rank of the error from smallest (1) to largest\n",
    "        self.prob = [1/(len(r_err)-i+1) for i in r_err]\n",
    "\n",
    "        \n",
    "    def priorized_sample(self,size):\n",
    "        prb = [i**self.alpha for i in self.prob]\n",
    "        t_s = [prb[0]]\n",
    "        for i in range(1,len(self.prob)):\n",
    "            t_s.append(prb[i]+t_s[i-1])\n",
    "        batch = []\n",
    "        mx_p = t_s[-1]\n",
    "        \n",
    "        smp_set = set()\n",
    "        \n",
    "        while len(smp_set)<batch_size:\n",
    "            tmp = np.random.uniform(0,mx_p)\n",
    "            for j in range(0, len(t_s)):\n",
    "                if t_s[j] > tmp:\n",
    "                    smp_set.add(max(j-1,0))\n",
    "                    break;\n",
    "        for i in smp_set:\n",
    "            batch.append([self.buffer[i], i])\n",
    "        return np.array(batch)\n",
    "#         return np.reshape(np.array(random.sample(self.buffer,size)),[size,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The code for the SUMO environment\n",
    "import os, sys\n",
    "# if 'SUMO_HOME' in os.environ:\n",
    "\n",
    "#The path of SUMO-tools to get the traci library\n",
    "sys.path.append(os.path.join('/home/ring/sumo-svn/', 'tools'))\n",
    "\n",
    "import traci\n",
    "import traci.constants as tc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "#Environment Model\n",
    "sumoBinary = \"/usr/local/bin/sumo\"\n",
    "sumoCmd = [sumoBinary, \"-c\", \"./cross/cross.sumo.cfg\"]  #The path to the sumo.cfg file\n",
    "\n",
    "#reset the environment\n",
    "def reset():\n",
    "    traci.start(sumoCmd)\n",
    "    tls = traci.trafficlights.getIDList()\n",
    "    return tls\n",
    "    \n",
    "#get the starting state\n",
    "def state():\n",
    "    for veh_id in traci.vehicle.getIDList():\n",
    "        traci.vehicle.subscribe(veh_id, (tc.VAR_POSITION, tc.VAR_SPEED))\n",
    "    p = traci.vehicle.getSubscriptionResults()\n",
    "    p_state = np.zeros((60,60,2))\n",
    "    for x in p:\n",
    "        ps = p[x][tc.VAR_POSITION]\n",
    "        spd = p[x][tc.VAR_SPEED]\n",
    "        p_state[int(ps[0]/5), int(ps[1]/5)] = [1, int(round(spd))]\n",
    "#         v_state[int(ps[0]/5), int(ps[1]/5)] = spd\n",
    "    p_state = np.reshape(p_state, [-1, 3600, 2])\n",
    "    return p_state #, v_state]\n",
    "\n",
    "#get the legal actions at the current phases of the traffic light\n",
    "def getLegalAction(phases):\n",
    "    legal_action = np.zeros(9)-1\n",
    "    i = 0\n",
    "    for x in phases:\n",
    "        if x>5:\n",
    "            legal_action[i] = i\n",
    "        if x<60:\n",
    "            legal_action[i+5] = i+5\n",
    "        i +=1\n",
    "    legal_action[4] = 4\n",
    "    return legal_action\n",
    "    \n",
    "#get the new phases after taking action from the current phases\n",
    "def getPhaseFromAction(phases, act):\n",
    "    if act<4:\n",
    "        phases[int(act)] -= 5\n",
    "    elif act>4:\n",
    "        phases[int(act)-5] += 5\n",
    "    return phases\n",
    "\n",
    "#the process of the action\n",
    "#input: traffic light; new phases; waiting time in the beginning of this cycle\n",
    "#output: new state; reward; End or not(Bool); new waiting time at the end of the next cycle\n",
    "def action(tls, ph, wait_time):  #parameters: the phase duration in the green signals\n",
    "    tls_id = tls[0]\n",
    "    init_p = traci.trafficlights.getPhase(tls_id) \n",
    "    prev = -1\n",
    "    changed = False\n",
    "    current_phases = ph\n",
    "    p_state = np.zeros((60,60,2))\n",
    "\n",
    "    step = 0\n",
    "    while traci.simulation.getMinExpectedNumber() > 0:\n",
    "        c_p = traci.trafficlights.getPhase(tls_id)\n",
    "        if c_p != prev and c_p%2==0:\n",
    "            traci.trafficlights.setPhaseDuration(tls_id, ph[int(c_p/2)]-0.5)\n",
    "            prev = c_p\n",
    "        if init_p != c_p:\n",
    "            changed = True\n",
    "        if changed:\n",
    "            if c_p == init_p:\n",
    "                break\n",
    "        traci.simulationStep()\n",
    "        step += 1\n",
    "        if step%10==0:\n",
    "            for veh_id in traci.vehicle.getIDList():\n",
    "                wait_time_map[veh_id] = traci.vehicle.getAccumulatedWaitingTime(veh_id)\n",
    "    for veh_id in traci.vehicle.getIDList():\n",
    "        traci.vehicle.subscribe(veh_id, (tc.VAR_POSITION, tc.VAR_SPEED, tc.VAR_ACCUMULATED_WAITING_TIME))\n",
    "    p = traci.vehicle.getSubscriptionResults()\n",
    "    \n",
    "    \n",
    "    wait_temp = dict(wait_time_map)\n",
    "    for x in p:\n",
    "        ps = p[x][tc.VAR_POSITION]\n",
    "        spd = p[x][tc.VAR_SPEED]\n",
    "        p_state[int(ps[0]/5), int(ps[1]/5)] = [1, int(round(spd))]\n",
    "\n",
    "    wait_t = sum(wait_temp[x] for x in wait_temp)\n",
    "    \n",
    "    d = False\n",
    "    if traci.simulation.getMinExpectedNumber() == 0:\n",
    "        d = True\n",
    "        \n",
    "    r = wait_time-wait_t\n",
    "    p_state = np.reshape(p_state, [-1, 3600, 2])\n",
    "    return p_state,r,d,wait_t\n",
    "\n",
    "#close the environment after every episode\n",
    "def end():\n",
    "    traci.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define the main QN and target QN\n",
    "mainQN = Qnetwork(h_size,np.int32(action_num))\n",
    "targetQN = Qnetwork(h_size,np.int32(action_num))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "#define the memory\n",
    "myBuffer0 = priorized_experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []  #number of steps in one episode\n",
    "rList = []  #reward in one episode\n",
    "wList = []  #the total waiting time in one episode\n",
    "awList = []  #the average waiting time in one episode\n",
    "tList = []   #thoughput in one episode (number of generated vehicles)\n",
    "nList = []   #stops' percentage (number of stopped vehicles divided by the total generated vehicles)\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "init_phases = [20,20,20,20]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#record the loss \n",
    "tf.summary.scalar('Loss', mainQN.loss)\n",
    "\n",
    "rfile = open(path+'/reward-rl.txt', 'w')\n",
    "wfile = open(path+'/wait-rl.txt', 'w')\n",
    "awfile = open(path+'/acc-wait-rl.txt', 'w')\n",
    "tfile = open(path+'/throput-rl.txt', 'w')\n",
    "\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "s_writer = tf.summary.FileWriter(path+'/train', sess.graph)\n",
    "s_writer.add_graph(sess.graph)\n",
    "    \n",
    "sess.run(init)\n",
    "tf.global_variables_initializer().run()\n",
    "if load_model == True:\n",
    "    print('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "updateTarget(targetOps,sess) #Set the target network to be equal to the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the running process of the total number of episodes\n",
    "for i in range(1,num_episodes):\n",
    "    episodeBuffer0 = priorized_experience_buffer()\n",
    "    #Reset environment and get first new observation\n",
    "    tls = reset()\n",
    "    s = state()#np.random.rand(1,10000)\n",
    "    current_phases = list(init_phases)\n",
    "    wait_time_map = {}\n",
    "    d = False\n",
    "    rAll = 0\n",
    "    j = 0\n",
    "    \n",
    "    print \"III:\", i, e\n",
    "    while j < max_epLength:\n",
    "        j+=1\n",
    "        \n",
    "        #get the legal actions at the current state\n",
    "        legal_action = getLegalAction(current_phases) #np.random.randint(1,action_num,size=action_num) #[1,2,-1,4,5]\n",
    "        \n",
    "        #Choose an action (0-8) by greedily (with e chance of random action) from the Q-network\n",
    "        if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "            a_cnd = [x for x in legal_action if x!=-1]\n",
    "            a_num = len(a_cnd)\n",
    "            a = np.random.randint(0, a_num)\n",
    "            a = a_cnd[a]\n",
    "        else:\n",
    "            np.reshape(s, [-1,3600,2])\n",
    "            legal_a_one = [0 if x!=-1 else -99999 for x in legal_action]\n",
    "            a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:s,mainQN.legal_actions:[legal_a_one]})[0]\n",
    "        \n",
    "        ph = getPhaseFromAction(current_phases,a)        \n",
    "        s1, r, d, wait_time = action(tls, ph, wait_time)\n",
    "        current_phases = ph\n",
    "\n",
    "        total_steps += 1\n",
    "        legal_a_one = [0 if x!=-1 else -99999 for x in legal_action] #the penalized Q value for illegal actions\n",
    "        legal_act_s1 = getLegalAction(ph)\n",
    "        legal_a_one_s1 = [0 if x!=-1 else -99999 for x in legal_act_s1]\n",
    "        episodeBuffer0.add(np.reshape(np.array([s,a,r,s1,d,legal_a_one, legal_a_one_s1]),[1,7])) #Save the experience to our episode buffer.\n",
    "\n",
    "        if total_steps > pre_train_steps:\n",
    "            if e > endE:\n",
    "                e -= stepDrop\n",
    "            if total_steps % (update_freq) == 0:\n",
    "                trainBatch = myBuffer0.priorized_sample(batch_size) #Get a random batch of experiences.\n",
    "                indx = np.reshape(np.vstack(trainBatch[:,1]), [batch_size])\n",
    "                indx = indx.astype(int)\n",
    "                trainBatch = np.vstack(trainBatch[:,0])\n",
    "\n",
    "                #Below we perform the Double-DQN update to the target Q-values \n",
    "                #action from the main QN\n",
    "                Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3]),mainQN.legal_actions:np.vstack(trainBatch[:,5])}) \n",
    "                #Q value from the target QN\n",
    "                Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3]), targetQN.legal_actions:np.vstack(trainBatch[:,6])})\n",
    "                # get targetQ at s'\n",
    "                end_multiplier = -(trainBatch[:,4] - 1)  #if end, 0; otherwise 1\n",
    "                doubleQ = Q2[range(batch_size),Q1]\n",
    "                targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "\n",
    "                #Update the network with our target values.\n",
    "                summry, err, ls, md = sess.run([merged, mainQN.td_error, mainQN.loss, mainQN.updateModel],  \\\n",
    "                    feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1],mainQN.legal_actions:np.vstack(trainBatch[:,5])})\n",
    "\n",
    "                s_writer.add_summary(summry, total_steps)\n",
    "                #update the target QN and the memory's prioritization\n",
    "                updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "                myBuffer0.updateErr(indx, err)\n",
    "                \n",
    "        rAll += r\n",
    "        s = s1\n",
    "\n",
    "        if d == True:\n",
    "            break\n",
    "    end()    \n",
    "\n",
    "    #save the data into the myBuffer\n",
    "    myBuffer0.add(episodeBuffer0.buffer)\n",
    "    \n",
    "    jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    rfile.write(str(rAll)+'\\n')\n",
    "    wt = sum(wait_time_map[x] for x in wait_time_map)\n",
    "    wtAve = wt/len(wait_time_map)\n",
    "    wList.append(wtAve)\n",
    "    wfile.write(str(wtAve)+'\\n')\n",
    "    awList.append(wt)\n",
    "    awfile.write(str(wt)+'\\n')\n",
    "    tList.append(len(wait_time_map))\n",
    "    tfile.write(str(len(wait_time_map))+'\\n')\n",
    "    tmp = [x for x in wait_time_map if wait_time_map[x] > 1]\n",
    "    nList.append(len(tmp)/len(wait_time_map))\n",
    "    print \"Total Reward---------------\",  rAll\n",
    "    #Periodically save the model. \n",
    "    if i % 100 == 0:\n",
    "        saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model\")\n",
    "#         if len(rList) % 10 == 0:\n",
    "#             print(total_steps,np.mean(rList[-10:]), e)\n",
    "saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save the data again in case data are missed in the previous step\n",
    "rfile = open(path+'/reward-rl.txt', 'w')\n",
    "wfile = open(path+'/wait-rl.txt', 'w')\n",
    "awfile = open(path+'/acc-wait-rl.txt', 'w')\n",
    "tfile = open(path+'/throput-rl.txt', 'w')\n",
    "jfile = open(path+'/epi-len-rl.txt', 'w')\n",
    "nfile = open(path+'/stop-rl.txt', 'w')\n",
    "\n",
    "for x in rList:\n",
    "    rfile.write(str(x)+'\\n')\n",
    "for x in wList:\n",
    "    wfile.write(str(x)+'\\n')\n",
    "for x in awList:\n",
    "    awfile.write(str(x)+'\\n')\n",
    "for x in tList:\n",
    "    tfile.write(str(x)+'\\n')\n",
    "for x in jList:\n",
    "    jfile.write(str(x)+'\\n')\n",
    "for x in nList:\n",
    "    nfile.write(str(x)+'\\n')\n",
    "   \n",
    "import matplotlib.pyplot as plt\n",
    "x =range(1, len(rList)+1)\n",
    "\n",
    "plt.scatter(x, rList)\n",
    "plt.show()\n",
    "plt.savefig(path+\"/reward.eps\", format=\"eps\")\n",
    "\n",
    "plt.scatter(x, wList)\n",
    "plt.show()\n",
    "plt.savefig(path+\"/wait.eps\", format=\"eps\")\n",
    "\n",
    "plt.scatter(x, awList)\n",
    "plt.show()\n",
    "plt.savefig(path+\"/acc-wait.eps\", format=\"eps\")\n",
    "\n",
    "plt.scatter(x, jList)\n",
    "plt.show()\n",
    "plt.savefig(path+\"/epi-len.eps\", format=\"eps\")\n",
    "\n",
    "plt.scatter(x, nList)\n",
    "plt.show()\n",
    "# print rList\n",
    "# print wList\n",
    "# print awList\n",
    "# print tList\n",
    "# print jList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
